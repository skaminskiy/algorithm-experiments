One divide and conquer algorithm for multiplying nxn matrices runs in Big Theta(n^3) time,
which is not better than the straightforward method of multiplying square matrices.
But the other, Strassen's algorithm, runs in O(n^2.81) time = O(n^lg7).

The most
asymptotically efficient algorithm for multiplying n 	 n matrices to date, due to
Coppersmith and Winograd [78], has a running time of O(n^2.376). The best lower
bound known is just the obvious .n2/ bound (obvious because we must fill in n2
elements of the product matrix).

From a practical point of view, Strassen’s algorithm is often not the method of
choice for matrix multiplication, for four reasons:
1. The constant factor hidden in the Big Theta(n^lg7) running time of Strassen’s algorithm
is larger than the constant factor in the Theta(n3)-time SQUARE-MATRIXMULTIPLY
procedure.
2. When the matrices are sparse, methods tailored for sparse matrices are faster.
3. Strassen’s algorithm is not quite as numerically stable as SQUARE-MATRIXMULTIPLY.
In other words, because of the limited precision of computer arithmetic
on noninteger values, larger errors accumulate in Strassen’s algorithm
than in SQUARE-MATRIX-MULTIPLY.
4. The submatrices formed at the levels of recursion consume space.

The latter two reasons were mitigated around 1990. Higham [167] demonstrated
that the difference in numerical stability had been overemphasized; although
Strassen’s algorithm is too numerically unstable for some applications, it is within
acceptable limits for others. Bailey, Lee, and Simon [32] discuss techniques for
reducing the memory requirements for Strassen’s algorithm.

In practice, fast matrix-multiplication implementations for dense matrices use
Strassen’s algorithm for matrix sizes above a “crossover point,” and they switch
to a simpler method once the subproblem size reduces to below the crossover
point. The exact value of the crossover point is highly system dependent.